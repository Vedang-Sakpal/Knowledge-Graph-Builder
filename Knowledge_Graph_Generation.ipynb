{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86999fd5",
   "metadata": {},
   "source": [
    "# Setup and Install Required Libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1718958",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rdflib owlready2 spacy transformers torch neo4j pandas markdown beautifulsoup4\n",
    "\n",
    "# Download spaCy language model\n",
    "%python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903804cd",
   "metadata": {},
   "source": [
    "# Section 2: Load OntoCAPE ontology \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854d4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes found: 783\n",
      "Example class names: ['N2491f9a091d646bc9bbc050271567271', 'ModelVariableSpecification', 'N17426ab650f745b390869d0b29163c5b', 'Exchange', 'Nc610ab085aa4431fb88992ebf13af6df', 'N3175ebab14504755933b55b25acc0e24', 'IntensiveThermodynamicStateVariable', 'Nec5bae67fa8b42239cf70136cd4443a2', 'SecondLevelSubsystem', 'PhaseInterfaceProperty']\n",
      "Total object properties found: 168\n",
      "Example property names: ['isDefinedBy', 'fulfills', 'isIndexOf', 'has_length', 'isInfluencedBy', 'indicatesMultiplicityOf', 'isPropertyOf', 'hasDirectSubsystem', 'hasFunctionalAspect', 'hasReaction']\n"
     ]
    }
   ],
   "source": [
    "'''from owlready2 import get_ontology\n",
    "\n",
    "onto = get_ontology(\"ontology\\OntoCAPE\\OntoCAPE.owl\").load()\n",
    "\n",
    "# Extract ontology class names and object properties\n",
    "onto_classes = list(onto.classes())\n",
    "onto_labels = [cls.name for cls in onto_classes]\n",
    "onto_obj_props = list(onto.object_properties())\n",
    "\n",
    "print(\"Example OntoCAPE Classes:\", onto_labels[:10])\n",
    "print(\"Example OntoCAPE Object Properties:\", [p.name for p in onto_obj_props[:10]])\n",
    "'''\n",
    "\n",
    "\n",
    "from rdflib import Graph, RDF, RDFS, OWL\n",
    "from urllib.parse import urlparse\n",
    "def extract_name(uri):\n",
    "    return uri.split('#')[-1] if '#' in uri else uri.split('/')[-1]\n",
    "    \n",
    "g = Graph()\n",
    "g.parse('ontology\\OntoCAPE\\OntoCAPE.owl') # Try 'xml' for RDF/XML\n",
    "\n",
    "\n",
    "owl_classes = set(g.subjects(RDF.type, OWL.Class))\n",
    "rdfs_classes = set(g.subjects(RDF.type, RDFS.Class))\n",
    "all_classes = owl_classes.union(rdfs_classes)\n",
    "class_labels = [extract_name(str(c)) for c in all_classes]\n",
    "print(f'Total classes found: {len(class_labels)}')\n",
    "print('Example class names:', class_labels[:10])\n",
    "obj_props = set(g.subjects(RDF.type, OWL.ObjectProperty))\n",
    "obj_labels = [extract_name(str(p)) for p in obj_props]\n",
    "print(f'Total object properties found: {len(obj_labels)}')\n",
    "print('Example property names:', obj_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f31e93",
   "metadata": {},
   "source": [
    "# Section 3: Load and Parse Process Description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e47356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crude Oil Production Unit (COPU) Process Description\n",
      "Overview\n",
      "The crude oil production unit is designed to process, separate, and condition crude oil from field production, separating associated gas and produced water, and preparing the oil for storage and export. The unit has a nominal capacity of 10 Mbpd (million barrels per day) and handles crude oil with an API gravity of 33°.\n",
      "Main Process Sections\n",
      "1. Incoming Crude Oil and Initial Separation\n",
      "\n",
      "Field Production Feed: \n",
      "Crude oil from wells arr\n"
     ]
    }
   ],
   "source": [
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load .md file\n",
    "with open(\"descriptions\\HAZOP_011_process_description.md\", \"r\") as f:\n",
    "    md_text = f.read()\n",
    "\n",
    "html = markdown.markdown(md_text)\n",
    "text = BeautifulSoup(html, \"html.parser\").get_text()\n",
    "\n",
    "print(text[:500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd1847",
   "metadata": {},
   "source": [
    "# Section 4: Extract Entities and Relations from Text\n",
    "We assume a basic rule-based or LLM-based approach here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e8714",
   "metadata": {},
   "source": [
    "## Using keyword based rules to extract entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c240db7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found equipment: pump in line -> The treated oil is stored in tanks before being pumped to the custody transfer point for export[1].\n",
      "Found equipment: pump in line -> The treated water is pumped and re-injected into a disposal well[1].\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example: rule-based extraction\n",
    "equipment_keywords = [\"reactor\", \"pump\", \"heat exchanger\", \"distillation\", \"compressor\", \"column\"]\n",
    "connections = []\n",
    "\n",
    "for line in text.split(\"\\n\"):\n",
    "    for kw in equipment_keywords:\n",
    "        if kw in line.lower():\n",
    "            print(f\"Found equipment: {kw} in line -> {line}\")\n",
    "\n",
    "# You can also extract relations manually or use spaCy/transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea311f",
   "metadata": {},
   "source": [
    "## Using Spacy/transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec3bb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\anaconda3\\envs\\KG_Builder_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: {'Tank', 'Pump'}\n",
      "Relations: [('Pump', 'TRANSFERS_TO', 'Tank'), ('Tank', 'TRANSFERS_TO', 'Pump')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load transformer for relation extraction\n",
    "relation_extractor = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Custom equipment types (can be extended)\n",
    "equipment_types = [\n",
    "    \"reactor\", \"heat exchanger\", \"pump\", \"distillation column\",\n",
    "    \"compressor\", \"valve\", \"vessel\", \"tank\"\n",
    "]\n",
    "\n",
    "# Entity & relation extraction\n",
    "doc = nlp(text)\n",
    "entities = set()\n",
    "relations = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    s_text = sent.text.strip()\n",
    "    for eq1 in equipment_types:\n",
    "        for eq2 in equipment_types:\n",
    "            if eq1 != eq2 and eq1 in s_text.lower() and eq2 in s_text.lower():\n",
    "                # Use zero-shot to extract relation type\n",
    "                candidate_labels = [\"feeds\", \"cools\", \"heats\", \"connects to\", \"transfers to\"]\n",
    "                result = relation_extractor(s_text, candidate_labels)\n",
    "                rel = result['labels'][0]\n",
    "                relations.append((eq1.title(), rel.replace(\" \", \"_\").upper(), eq2.title()))\n",
    "                entities.add(eq1.title())\n",
    "                entities.add(eq2.title())\n",
    "\n",
    "print(\"Entities:\", entities)\n",
    "print(\"Relations:\", relations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598223c9",
   "metadata": {},
   "source": [
    "# Section 5: Map to OntoCAPE Ontology Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ecf6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each entity to an OntoCAPE class\n",
    "equipment_to_ontocape = {\n",
    "    \"Reactor\": \"ontoCAPE.ChemicalReactor\",\n",
    "    \"Heat Exchanger\": \"ontoCAPE.HeatExchanger\",\n",
    "    \"Pump\": \"ontoCAPE.Pump\",\n",
    "    \"Distillation Column\": \"ontoCAPE.DistillationColumn\",\n",
    "    \"Compressor\": \"ontoCAPE.Compressor\",\n",
    "    \"Valve\": \"ontoCAPE.Valve\",\n",
    "    \"Vessel\": \"ontoCAPE.Vessel\",\n",
    "    \"Tank\": \"ontoCAPE.Tank\"\n",
    "}\n",
    "\n",
    "nodes = [(ent, equipment_to_ontocape.get(ent, \"ontoCAPE.Equipment\")) for ent in entities]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf6b15",
   "metadata": {},
   "source": [
    "# Section 6: Build Graph Model (Nodes + Relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb46e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cypher_node(name, label):\n",
    "    return f\"MERGE (:{label.split('.')[-1]} {{name: '{name}'}})\"\n",
    "\n",
    "def to_cypher_relation(src, rel, tgt):\n",
    "    return f\"MATCH (a {{name: '{src}'}}), (b {{name: '{tgt}'}}) MERGE (a)-[:{rel}]->(b)\"\n",
    "\n",
    "cypher_nodes = [to_cypher_node(n, l) for n, l in nodes]\n",
    "cypher_edges = [to_cypher_relation(s, r, t) for s, r, t in relations]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9b7315",
   "metadata": {},
   "source": [
    "# Section 7: Generate Cypher Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27b969b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGE (:Tank {name: 'Tank'})\n",
      "MERGE (:Pump {name: 'Pump'})\n",
      "MATCH (a {name: 'Pump'}), (b {name: 'Tank'}) MERGE (a)-[:TRANSFERS_TO]->(b)\n",
      "MATCH (a {name: 'Tank'}), (b {name: 'Pump'}) MERGE (a)-[:TRANSFERS_TO]->(b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Save generated Cypher code\\nwith open(\"generated_kg.cypher\", \"w\") as f:\\n    f.write(\"// Nodes\\n\")\\n    f.write(\"\\n\".join(cypher_nodes))\\n    f.write(\"\\n\\n// Relationships\\n\")\\n    f.write(\"\\n\".join(cypher_edges))\\n\\nprint(\"Cypher file \\'generated_kg.cypher\\' saved successfully.\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_cypher_node(name, label):\n",
    "    return f\"MERGE (:{label} {{name: '{name}'}})\"\n",
    "\n",
    "def to_cypher_relation(src, rel, tgt):\n",
    "    return f\"MATCH (a {{name: '{src}'}}), (b {{name: '{tgt}'}}) MERGE (a)-[:{rel.upper()}]->(b)\"\n",
    "\n",
    "cypher_nodes = [to_cypher_node(n, l.split(\".\")[-1]) for n, l in nodes]\n",
    "cypher_edges = [to_cypher_relation(s, r, t) for s, r, t in relations]\n",
    "\n",
    "# Print Cypher code\n",
    "for line in cypher_nodes + cypher_edges:\n",
    "    print(line)\n",
    "\n",
    "'''\n",
    "# Save generated Cypher code\n",
    "with open(\"generated_kg.cypher\", \"w\") as f:\n",
    "    f.write(\"// Nodes\\n\")\n",
    "    f.write(\"\\n\".join(cypher_nodes))\n",
    "    f.write(\"\\n\\n// Relationships\\n\")\n",
    "    f.write(\"\\n\".join(cypher_edges))\n",
    "\n",
    "print(\"Cypher file 'generated_kg.cypher' saved successfully.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3292383b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q neo4j pandas networkx lxml xlsxwriter openai sentence-transformers faiss-cpu tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a1321",
   "metadata": {},
   "source": [
    "# Automation of HAZOP analysis \n",
    "\n",
    "using RAG with the normalised causes,consequences, and safeguards as a databse and generatinh HAZOP KG and excel reoprt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# This script orchestrates the entire automated HAZOP generation pipeline.\n",
    "# It calls the individual modules in the correct sequence to perform\n",
    "# each step of the process, from initial data ingestion to final report generation.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import config\n",
    "\n",
    "def run_script(script_name):\n",
    "    \"\"\"Helper function to run a python script and handle errors.\"\"\"\n",
    "    print(f\"--- Running {script_name} ---\")\n",
    "    try:\n",
    "        # We use subprocess to run each script as a separate process.\n",
    "        # This ensures a clean state for each step of the pipeline.\n",
    "        result = subprocess.run(['python', script_name], check=True, capture_output=True, text=True)\n",
    "        print(result.stdout)\n",
    "        print(f\"--- Finished {script_name} successfully ---\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"!!! Error running {script_name} !!!\")\n",
    "        print(e.stderr)\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(f\"!!! Error: {script_name} not found. Make sure all scripts are in the same directory. !!!\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the HAZOP automation pipeline.\"\"\"\n",
    "    print(\"======================================================\")\n",
    "    print(\"  Automated HAZOP Generation Framework - Pipeline     \")\n",
    "    print(\"======================================================\")\n",
    "    print(\"\\nNOTE: This pipeline assumes you have already loaded your\\nP&ID data into the Neo4j database.\\n\")\n",
    "\n",
    "    # Define the sequence of scripts to be executed\n",
    "    # 01_pid_to_graph.py has been removed as per user request.\n",
    "    pipeline_scripts = [\n",
    "        \"02_semantic_enrichment.py\",\n",
    "        \"03_manual_noding.py\",\n",
    "        \"04_hazop_analysis_engine.py\",\n",
    "        \"05_report_generator.py\"\n",
    "    ]\n",
    "\n",
    "    # Execute each script in the pipeline\n",
    "    for script in pipeline_scripts:\n",
    "        if not run_script(script):\n",
    "            print(f\"\\nPipeline stopped due to an error in {script}.\")\n",
    "            break\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"======================================================\")\n",
    "    print(\"          Pipeline execution complete.                \")\n",
    "    print(\"======================================================\")\n",
    "    print(f\"Check the Neo4j database for the HAZOP Knowledge Graph.\")\n",
    "    print(f\"Check the project directory for the generated '{config.EXCEL_REPORT_PATH}' file.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure necessary files exist before starting\n",
    "    # DEXPI_FILE_PATH check removed.\n",
    "    required_files = [\n",
    "        config.PROCESS_DESC_PATH,\n",
    "        config.MSDS_FILE_PATH,\n",
    "        config.CAUSES_CSV_PATH,\n",
    "        config.CONSEQUENCES_CSV_PATH,\n",
    "        config.SAFEGUARDS_CSV_PATH,\n",
    "        config.PARAMETER_GUIDEWORD_CSV_PATH\n",
    "    ]\n",
    "    \n",
    "    files_missing = False\n",
    "    for f in required_files:\n",
    "        if not os.path.exists(f):\n",
    "            print(f\"Error: Required file not found at path: {f}\")\n",
    "            files_missing = True\n",
    "            \n",
    "    if not files_missing:\n",
    "        main()\n",
    "    else:\n",
    "        print(\"\\nPlease ensure all required files are present and paths in config.py are correct.\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "'''\n",
    "# config.py\n",
    "# Configuration file for the Automated HAZOP Generation Framework.\n",
    "# Store all sensitive information and file paths here.\n",
    "# IMPORTANT: Fill in your actual credentials and paths before running the scripts.\n",
    "\n",
    "# --- Neo4j Database Configuration ---\n",
    "# Your Neo4j AuraDB URI, username, and password.\n",
    "# Example URI: \"neo4j+s://xxxxxxxx.databases.neo4j.io\"\n",
    "NEO4J_URI = \"YOUR_NEO4J_URI\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"YOUR_NEO4J_PASSWORD\"\n",
    "\n",
    "# --- Large Language Model (LLM) API Configuration ---\n",
    "# Using OpenAI's API as an example.\n",
    "# Replace with your API key.\n",
    "LLM_API_KEY = \"YOUR_OPENAI_API_KEY\"\n",
    "# Specify the model you want to use. gpt-4-turbo is recommended for its reasoning capabilities.\n",
    "LLM_MODEL_NAME = \"gpt-4-turbo\"\n",
    "\n",
    "# --- File Paths Configuration ---\n",
    "# Paths to all input files required by the pipeline.\n",
    "# Ensure these files are in your project directory or provide absolute paths.\n",
    "\n",
    "# Path to the process description markdown file.\n",
    "PROCESS_DESC_PATH = \"path/to/your/process_description.md\" # Replace with your actual file path\n",
    "\n",
    "# Path to the Material Safety Data Sheet (MSDS) file.\n",
    "MSDS_FILE_PATH = \"path/to/your/msds.txt\" # Replace with your actual file path\n",
    "\n",
    "# Paths to the reference CSV files containing historical HAZOP data.\n",
    "CAUSES_CSV_PATH = \"Causes.csv\"\n",
    "CONSEQUENCES_CSV_PATH = \"Consequences.csv\"\n",
    "SAFEGUARDS_CSV_PATH = \"Safeguards.csv\"\n",
    "\n",
    "# Path to the CSV file defining parameters and their applicable guidewords.\n",
    "PARAMETER_GUIDEWORD_CSV_PATH = \"Parameter and Guideword.csv\"\n",
    "\n",
    "# --- Output File Path Configuration ---\n",
    "# Path for the generated Excel HAZOP report.\n",
    "EXCEL_REPORT_PATH = \"HAZOP_Report_Generated.xlsx\"\n",
    "\n",
    "# --- HAZOP Analysis Parameters ---\n",
    "# Confidence score threshold for highlighting in the Excel report.\n",
    "CONFIDENCE_THRESHOLD = 0.75\n",
    "\n",
    "# Hardcoded lists for guidewords and parameters have been removed.\n",
    "# The system now reads them from PARAMETER_GUIDEWORD_CSV_PATH.\n",
    "'''\n",
    "# --------------------------------------------------------------------------\n",
    "'''\n",
    "# requirements.txt\n",
    "# List of Python packages required to run the HAZOP automation scripts.\n",
    "# Install these packages using pip:\n",
    "# pip install -r requirements.txt\n",
    "\n",
    "# For connecting to and interacting with the Neo4j database.\n",
    "neo4j\n",
    "\n",
    "# For handling data in tabular format, especially for CSVs and Excel generation.\n",
    "pandas\n",
    "\n",
    "# For creating and manipulating the graph structure in memory before loading to Neo4j.\n",
    "networkx\n",
    "\n",
    "# For parsing the DEXPI XML file.\n",
    "lxml\n",
    "\n",
    "# For generating formatted Excel files. Works with pandas.\n",
    "xlsxwriter\n",
    "\n",
    "# For making API calls to the Large Language Model (e.g., OpenAI).\n",
    "openai\n",
    "\n",
    "# For creating vector embeddings for semantic search.\n",
    "sentence-transformers\n",
    "\n",
    "# For efficient similarity search in the RAG step.\n",
    "faiss-cpu\n",
    "\n",
    "# For progress bars to monitor long-running processes.\n",
    "tqdm\n",
    "'''\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# 02_semantic_enrichment.py\n",
    "# This script handles the semantic enrichment of the Knowledge Graph.\n",
    "# It uses an LLM to parse unstructured documents (process description, MSDS)\n",
    "# and adds the extracted information as properties to the existing nodes in Neo4j.\n",
    "\n",
    "import config\n",
    "from neo4j import GraphDatabase\n",
    "import openai\n",
    "import json\n",
    "\n",
    "class SemanticEnricher:\n",
    "    \"\"\"\n",
    "    A class to enrich the Process KG with data from documents using an LLM.\n",
    "    \"\"\"\n",
    "    def __init__(self, uri, user, password, api_key):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        openai.api_key = api_key\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def get_text_from_file(self, file_path):\n",
    "        \"\"\"Reads content from a text file.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found at {file_path}\")\n",
    "            return None\n",
    "\n",
    "    def extract_info_with_llm(self, text_content, prompt_template):\n",
    "        \"\"\"\n",
    "        Uses an LLM to extract structured information from text.\n",
    "        \"\"\"\n",
    "        print(\"Contacting LLM for information extraction...\")\n",
    "        try:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=config.LLM_MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a data extraction expert. Your task is to read the provided text and extract information in a structured JSON format. Do not add any explanatory text outside of the JSON object.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"{prompt_template}\\n\\n---DOCUMENT TEXT---\\n{text_content}\"}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            extracted_json = json.loads(response.choices[0].message.content)\n",
    "            print(\"Successfully extracted information from LLM.\")\n",
    "            return extracted_json\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during LLM API call: {e}\")\n",
    "            return None\n",
    "\n",
    "    def enrich_graph(self, extracted_data):\n",
    "        \"\"\"\n",
    "        Adds the extracted information as properties to nodes in the Neo4j graph.\n",
    "        \"\"\"\n",
    "        print(\"Enriching the Neo4j graph with extracted data...\")\n",
    "        with self.driver.session() as session:\n",
    "            # Enrich equipment with process parameters\n",
    "            if 'equipment_parameters' in extracted_data:\n",
    "                for item in extracted_data['equipment_parameters']:\n",
    "                    # The user's graph uses 'name' as the identifier.\n",
    "                    name = item.get('name')\n",
    "                    params = item.get('parameters', {})\n",
    "                    if name:\n",
    "                        query = f\"\"\"\n",
    "                        MATCH (n {{name: $name}})\n",
    "                        SET n += $params\n",
    "                        \"\"\"\n",
    "                        session.run(query, name=name, params=params)\n",
    "                        print(f\"Enriched component '{name}' with parameters.\")\n",
    "\n",
    "            # Add chemical data and link to vessels\n",
    "            if 'chemical_data' in extracted_data:\n",
    "                for chem in extracted_data['chemical_data']:\n",
    "                    name = chem.get('chemical_name')\n",
    "                    properties = chem.get('properties', {})\n",
    "                    # The user's graph uses 'name' as the identifier.\n",
    "                    vessel_name = chem.get('stored_in_vessel_name')\n",
    "                    if name:\n",
    "                        # Create or merge the chemical node\n",
    "                        session.run(\"MERGE (c:Chemical {name: $name}) SET c += $props\", name=name, props=properties)\n",
    "                        print(f\"Added/Updated chemical: {name}\")\n",
    "                        # Link chemical to the vessel it's stored in\n",
    "                        if vessel_name:\n",
    "                            session.run(\"\"\"\n",
    "                                MATCH (v {{name: $vessel_name}})\n",
    "                                MATCH (c:Chemical {{name: $chem_name}})\n",
    "                                MERGE (v)-[:CONTAINS]->(c)\n",
    "                            \"\"\", vessel_name=vessel_name, chem_name=name)\n",
    "                            print(f\"Linked '{name}' to vessel '{vessel_name}'.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function for Phase 2.\"\"\"\n",
    "    enricher = SemanticEnricher(config.NEO4J_URI, config.NEO4J_USERNAME, config.NEO4J_PASSWORD, config.LLM_API_KEY)\n",
    "\n",
    "    # --- Process Description Enrichment ---\n",
    "    process_desc_text = enricher.get_text_from_file(config.PROCESS_DESC_PATH)\n",
    "    if process_desc_text:\n",
    "        process_prompt = \"\"\"\n",
    "        From the process description text provided, extract the operating parameters for each piece of major equipment.\n",
    "        Identify equipment by its name (e.g., 'V-001', 'P-203').\n",
    "        For each piece of equipment, extract parameters like 'operatingPressure', 'operatingTemperature', and 'designPressure'.\n",
    "        \n",
    "        Format the output as a JSON object with a single key \"equipment_parameters\", which is a list of objects.\n",
    "        Each object in the list should have a \"name\" and a \"parameters\" object.\n",
    "        \n",
    "        Example JSON format:\n",
    "        {\n",
    "          \"equipment_parameters\": [\n",
    "            {\n",
    "              \"name\": \"V-002\",\n",
    "              \"parameters\": {\n",
    "                \"operatingPressure\": \"10 barg\",\n",
    "                \"operatingTemperature\": \"150 C\",\n",
    "                \"designPressure\": \"15 barg\"\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        extracted_process_data = enricher.extract_info_with_llm(process_desc_text, process_prompt)\n",
    "        if extracted_process_data:\n",
    "            enricher.enrich_graph(extracted_process_data)\n",
    "\n",
    "    # --- MSDS Enrichment ---\n",
    "    msds_text = enricher.get_text_from_file(config.MSDS_FILE_PATH)\n",
    "    if msds_text:\n",
    "        msds_prompt = \"\"\"\n",
    "        From the Material Safety Data Sheet (MSDS) text provided, extract key safety and physical properties for the chemical.\n",
    "        Also, identify which vessel name it is primarily stored or used in, if mentioned.\n",
    "        \n",
    "        Format the output as a JSON object with a single key \"chemical_data\", which is a list of objects.\n",
    "        Each object should have \"chemical_name\", \"stored_in_vessel_name\", and a \"properties\" object.\n",
    "        \n",
    "        Example JSON format:\n",
    "        {\n",
    "          \"chemical_data\": [\n",
    "            {\n",
    "              \"chemical_name\": \"Methanol\",\n",
    "              \"stored_in_vessel_name\": \"V-001\",\n",
    "              \"properties\": {\n",
    "                \"flashPoint\": \"11 C\",\n",
    "                \"boilingPoint\": \"64.7 C\",\n",
    "                \"healthHazard\": \"Toxic if swallowed or inhaled\"\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        extracted_msds_data = enricher.extract_info_with_llm(msds_text, msds_prompt)\n",
    "        if extracted_msds_data:\n",
    "            enricher.enrich_graph(extracted_msds_data)\n",
    "\n",
    "    enricher.close()\n",
    "    print(\"Phase 2: Semantic Enrichment complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# 03_manual_noding.py\n",
    "# This script allows the user to manually select HAZOP nodes.\n",
    "# It queries all components from the graph, presents them to the user,\n",
    "# and creates HAZOPNode entities based on the user's selection.\n",
    "\n",
    "import config\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class ManualNoder:\n",
    "    \"\"\"\n",
    "    A class to facilitate manual selection of HAZOP nodes.\n",
    "    \"\"\"\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def get_all_components(self):\n",
    "        \"\"\"Fetches all potential components for HAZOP analysis from the graph.\"\"\"\n",
    "        print(\"Fetching all components from the graph...\")\n",
    "        with self.driver.session() as session:\n",
    "            # This query fetches any node with a 'name' property.\n",
    "            # This is based on the user-provided Cypher query.\n",
    "            query = \"\"\"\n",
    "            MATCH (n) WHERE exists(n.name)\n",
    "            RETURN n.name AS name, labels(n)[0] AS type\n",
    "            ORDER BY type, name\n",
    "            \"\"\"\n",
    "            result = session.run(query)\n",
    "            return [record.data() for record in result]\n",
    "\n",
    "    def select_nodes(self, components):\n",
    "        \"\"\"Presents components to the user and gets their selection.\"\"\"\n",
    "        if not components:\n",
    "            print(\"No components found in the database. Please load your P&ID graph first.\")\n",
    "            return []\n",
    "\n",
    "        print(\"\\nPlease select the components to be treated as HAZOP Nodes:\")\n",
    "        for i, comp in enumerate(components):\n",
    "            print(f\"  [{i+1}] {comp['name']} (Type: {comp['type']})\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                selection_str = input(\"\\nEnter the numbers of the nodes to analyze, separated by commas (e.g., 1, 3, 5): \")\n",
    "                selected_indices = [int(s.strip()) - 1 for s in selection_str.split(',')]\n",
    "                \n",
    "                selected_components = []\n",
    "                for i in selected_indices:\n",
    "                    if 0 <= i < len(components):\n",
    "                        selected_components.append(components[i])\n",
    "                    else:\n",
    "                        print(f\"Warning: Index {i+1} is out of range and will be ignored.\")\n",
    "                \n",
    "                if selected_components:\n",
    "                    print(\"\\nYou have selected the following nodes for HAZOP analysis:\")\n",
    "                    for comp in selected_components:\n",
    "                        print(f\"  - {comp['name']}\")\n",
    "                    confirm = input(\"Is this correct? (yes/no): \").lower()\n",
    "                    if confirm == 'yes':\n",
    "                        return selected_components\n",
    "                else:\n",
    "                    print(\"No valid nodes selected. Please try again.\")\n",
    "\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter numbers separated by commas.\")\n",
    "\n",
    "    def create_hazop_nodes(self, selected_components):\n",
    "        \"\"\"Creates HAZOPNode entities in the graph for the selected components.\"\"\"\n",
    "        print(\"\\nCreating HAZOPNode entities in the graph...\")\n",
    "        with self.driver.session() as session:\n",
    "            # First, ensure no old HAZOP nodes exist to avoid confusion\n",
    "            session.run(\"MATCH (n:HAZOPNode) DETACH DELETE n\")\n",
    "            print(\"Cleared any pre-existing HAZOP nodes.\")\n",
    "\n",
    "            for comp in selected_components:\n",
    "                comp_name = comp['name']\n",
    "                query = \"\"\"\n",
    "                MATCH (c {name: $comp_name})\n",
    "                MERGE (n:HAZOPNode {nodeID: 'Node-' + $comp_name, description: 'Node for ' + labels(c)[0] + ' ' + $comp_name})\n",
    "                MERGE (n)-[:ANALYZES]->(c)\n",
    "                \"\"\"\n",
    "                session.run(query, comp_name=comp_name)\n",
    "            print(f\"Successfully created {len(selected_components)} HAZOPNode entities.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function for Phase 3.\"\"\"\n",
    "    noder = ManualNoder(config.NEO4J_URI, config.NEO4J_USERNAME, config.NEO4J_PASSWORD)\n",
    "    all_components = noder.get_all_components()\n",
    "    selected = noder.select_nodes(all_components)\n",
    "    if selected:\n",
    "        noder.create_hazop_nodes(selected)\n",
    "    else:\n",
    "        print(\"No nodes were selected. Halting pipeline.\")\n",
    "        # Exit gracefully if no nodes are chosen\n",
    "        exit()\n",
    "    noder.close()\n",
    "    print(\"Phase 3: Manual Noding complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# 04_hazop_analysis_engine.py\n",
    "# This is the core of the framework. It performs the LLM-powered HAZOP analysis.\n",
    "# - Systematically generates deviations for each HAZOP node based on a CSV file.\n",
    "# - Uses Retrieval-Augmented Generation (RAG) to assemble context.\n",
    "# - Prompts the LLM for causal analysis.\n",
    "# - Simulates confidence scoring.\n",
    "# - Populates the results back into the Neo4j Knowledge Graph.\n",
    "\n",
    "import config\n",
    "from neo4j import GraphDatabase\n",
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DeviationGenerator:\n",
    "    \"\"\"Generates deviations based on the 'Parameter and Guideword.csv' file.\"\"\"\n",
    "    def __init__(self, csv_path):\n",
    "        print(\"Initializing Deviation Generator...\")\n",
    "        self.param_guideword_map = self._load_param_guideword_map(csv_path)\n",
    "        print(\"Deviation Generator initialized successfully.\")\n",
    "\n",
    "    def _load_param_guideword_map(self, csv_path):\n",
    "        \"\"\"\n",
    "        Loads and parses the CSV to map parameters to applicable guidewords.\n",
    "        This version is designed for a clean, matrix-style CSV.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            # The first column is the parameter, the rest are guidewords.\n",
    "            param_col = df.columns[0]\n",
    "            guideword_cols = df.columns[1:]\n",
    "            \n",
    "            df = df.set_index(param_col)\n",
    "            \n",
    "            param_map = {}\n",
    "            for param, row in df.iterrows():\n",
    "                # Clean up parameter name (remove trailing spaces, etc.)\n",
    "                param_clean = str(param).strip()\n",
    "                if param_clean:\n",
    "                    # An applicable guideword is a column where the cell value contains '✔'\n",
    "                    applicable_guidewords = [gw for gw in guideword_cols if pd.notna(row[gw]) and '✔' in str(row[gw])]\n",
    "                    param_map[param_clean] = applicable_guidewords\n",
    "            \n",
    "            print(f\"Loaded {len(param_map)} parameters from CSV.\")\n",
    "            return param_map\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or parsing {csv_path}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def generate_deviations_for_node(self, node_id):\n",
    "        \"\"\"Generates all valid deviations for a given node.\"\"\"\n",
    "        deviations = []\n",
    "        for param, guidewords in self.param_guideword_map.items():\n",
    "            for guideword in guidewords:\n",
    "                deviation_desc = f\"{guideword} {param}\"\n",
    "                deviation_id = f\"{node_id}-{guideword.replace(' ','')}-{param.replace(' ','')}\"\n",
    "                deviations.append({\n",
    "                    \"deviationID\": deviation_id,\n",
    "                    \"description\": deviation_desc,\n",
    "                    \"guideword\": guideword,\n",
    "                    \"parameter\": param,\n",
    "                    \"nodeID\": node_id\n",
    "                })\n",
    "        return deviations\n",
    "\n",
    "class RAGContextBuilder:\n",
    "    \"\"\"Builds the context for the RAG prompt.\"\"\"\n",
    "    def __init__(self):\n",
    "        print(\"Initializing RAG Context Builder...\")\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.cause_df = pd.read_csv(config.CAUSES_CSV_PATH).dropna(how='all').stack().reset_index(drop=True).to_frame('description')\n",
    "        self.consequence_df = pd.read_csv(config.CONSEQUENCES_CSV_PATH).dropna(how='all').stack().reset_index(drop=True).to_frame('description')\n",
    "        self.safeguard_df = pd.read_csv(config.SAFEGUARDS_CSV_PATH).dropna(how='all').stack().reset_index(drop=True).to_frame('description')\n",
    "        \n",
    "        self.cause_embeddings = self.model.encode(self.cause_df['description'].tolist())\n",
    "        self.consequence_embeddings = self.model.encode(self.consequence_df['description'].tolist())\n",
    "        self.safeguard_embeddings = self.model.encode(self.safeguard_df['description'].tolist())\n",
    "\n",
    "        self.cause_index = self._create_faiss_index(self.cause_embeddings)\n",
    "        self.consequence_index = self._create_faiss_index(self.consequence_embeddings)\n",
    "        self.safeguard_index = self._create_faiss_index(self.safeguard_embeddings)\n",
    "        print(\"RAG Context Builder initialized successfully.\")\n",
    "\n",
    "    def _create_faiss_index(self, embeddings):\n",
    "        index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        index.add(np.array(embeddings, dtype=np.float32))\n",
    "        return index\n",
    "\n",
    "    def search(self, query, k=3):\n",
    "        \"\"\"Performs semantic search to find relevant historical data.\"\"\"\n",
    "        query_embedding = self.model.encode([query])\n",
    "        \n",
    "        _, cause_indices = self.cause_index.search(np.array(query_embedding, dtype=np.float32), k)\n",
    "        retrieved_causes = self.cause_df.iloc[cause_indices[0]]['description'].tolist()\n",
    "\n",
    "        _, cons_indices = self.consequence_index.search(np.array(query_embedding, dtype=np.float32), k)\n",
    "        retrieved_consequences = self.consequence_df.iloc[cons_indices[0]]['description'].tolist()\n",
    "\n",
    "        _, safe_indices = self.safeguard_index.search(np.array(query_embedding, dtype=np.float32), k)\n",
    "        retrieved_safeguards = self.safeguard_df.iloc[safe_indices[0]]['description'].tolist()\n",
    "\n",
    "        return {\n",
    "            \"causes\": retrieved_causes,\n",
    "            \"consequences\": retrieved_consequences,\n",
    "            \"safeguards\": retrieved_safeguards\n",
    "        }\n",
    "\n",
    "class AnalysisEngine:\n",
    "    \"\"\"The main engine for performing the HAZOP analysis.\"\"\"\n",
    "    def __init__(self, uri, user, password, api_key):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        openai.api_key = api_key\n",
    "        self.rag_builder = RAGContextBuilder()\n",
    "        self.deviation_generator = DeviationGenerator(config.PARAMETER_GUIDEWORD_CSV_PATH)\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def get_hazop_nodes(self):\n",
    "        \"\"\"Fetches all identified HAZOP nodes from the graph.\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"MATCH (n:HAZOPNode) RETURN n.nodeID AS nodeID, n.description AS description\")\n",
    "            return [record.data() for record in result]\n",
    "\n",
    "    def get_graph_context(self, node_id):\n",
    "        \"\"\"Retrieves the local topology for a given HAZOP node.\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # This query is adapted for the user's schema\n",
    "            query = \"\"\"\n",
    "            MATCH (n:HAZOPNode {nodeID: $node_id})-[:ANALYZES]->(comp)\n",
    "            OPTIONAL MATCH path = (comp)-[:Connected_to*1..2]-(neighbor)\n",
    "            WITH n, comp, collect(DISTINCT {\n",
    "                name: neighbor.name, \n",
    "                type: labels(neighbor)[0], \n",
    "                opPressure: neighbor.operatingPressure\n",
    "            }) AS neighbors\n",
    "            RETURN 'Component ' + comp.name + ' (' + labels(comp)[0] + ') is part of ' + n.description + '. ' +\n",
    "                   'It is connected to: ' + apoc.convert.toString(neighbors) AS context\n",
    "            \"\"\"\n",
    "            result = session.run(query, node_id=node_id)\n",
    "            record = result.single()\n",
    "            return record['context'] if record else \"No specific graph context found.\"\n",
    "\n",
    "    def perform_analysis(self, deviation):\n",
    "        \"\"\"Performs the full RAG and LLM analysis for a single deviation.\"\"\"\n",
    "        graph_context = self.get_graph_context(deviation['nodeID'])\n",
    "        rag_context = self.rag_builder.search(f\"{deviation['description']} in {deviation['nodeID']}\")\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a world-class Process Safety Engineer conducting a HAZOP study.\n",
    "        Your task is to analyze the following process deviation.\n",
    "        \n",
    "        **HAZOP Node:** {deviation['nodeID']}\n",
    "        **Deviation to Analyze:** {deviation['description']}\n",
    "        \n",
    "        **Context from P&ID and Process KG:**\n",
    "        {graph_context}\n",
    "        \n",
    "        **Context from Historical HAZOP Data (Reference Files):**\n",
    "        - Similar Causes Found: {rag_context['causes']}\n",
    "        - Similar Consequences Found: {rag_context['consequences']}\n",
    "        - Similar Safeguards Found: {rag_context['safeguards']}\n",
    "\n",
    "        **Your Task:**\n",
    "        1. Identify all credible **Causes** for this deviation.\n",
    "        2. Identify all potential **Consequences**.\n",
    "        3. Identify all existing **Safeguards**.\n",
    "        4. **Critical Instruction:** For every item, provide a `source` tag.\n",
    "           - If from context, cite the source (e.g., `source: \"Causes.csv\"`, `source: \"P&ID\"`).\n",
    "           - If you generate a novel item based on your engineering reasoning, label it `source: \"LLM-Inferred\"`.\n",
    "        5. Format your complete output as a single, valid JSON object with three keys: \"causes\", \"consequences\", and \"safeguards\". \n",
    "           Each key should contain a list of objects, where each object has \"description\" and \"source\" fields.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=config.LLM_MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a process safety expert. Output only valid JSON.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            analysis_result = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            for category in analysis_result:\n",
    "                for item in analysis_result[category]:\n",
    "                    item['confidenceLevel'] = np.random.uniform(0.6, 1.0)\n",
    "            \n",
    "            return analysis_result\n",
    "        except Exception as e:\n",
    "            print(f\"Error during LLM analysis for {deviation['deviationID']}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def populate_graph_with_analysis(self, deviation, analysis_result):\n",
    "        \"\"\"Writes the analysis results back to the Neo4j graph.\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(\"\"\"\n",
    "                MATCH (n:HAZOPNode {nodeID: $nodeID})\n",
    "                MERGE (d:Deviation {deviationID: $devID})\n",
    "                ON CREATE SET d.description = $desc, d.guideword = $gw, d.parameter = $p\n",
    "                MERGE (n)-[:HAS_DEVIATION]->(d)\n",
    "            \"\"\", nodeID=deviation['nodeID'], devID=deviation['deviationID'], desc=deviation['description'],\n",
    "                 gw=deviation['guideword'], p=deviation['parameter'])\n",
    "\n",
    "            for category in ['causes', 'consequences', 'safeguards']:\n",
    "                node_label = category.capitalize()[:-1]\n",
    "                for item in analysis_result.get(category, []):\n",
    "                    query = f\"\"\"\n",
    "                    MATCH (d:Deviation {{deviationID: $devID}})\n",
    "                    MERGE (item:{node_label} {{description: $desc}})\n",
    "                    ON CREATE SET item.source = $source, item.confidenceLevel = $conf, item.validationStatus = 'Unreviewed'\n",
    "                    \"\"\"\n",
    "                    if node_label == 'Cause':\n",
    "                        query += \" MERGE (item)-[:CAUSES]->(d)\"\n",
    "                    elif node_label == 'Consequence':\n",
    "                        query += \" MERGE (d)-[:LEADS_TO]->(item)\"\n",
    "                    elif node_label == 'Safeguard':\n",
    "                        query += \" MERGE (item)-[:PROTECTS_AGAINST]->(d)\"\n",
    "                    \n",
    "                    session.run(query, devID=deviation['deviationID'], **item)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function for Phase 4.\"\"\"\n",
    "    engine = AnalysisEngine(config.NEO4J_URI, config.NEO4J_USERNAME, config.NEO4J_PASSWORD, config.LLM_API_KEY)\n",
    "    \n",
    "    hazop_nodes = engine.get_hazop_nodes()\n",
    "    if not hazop_nodes:\n",
    "        print(\"No HAZOP nodes found. Please run the manual noding script (03) first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(hazop_nodes)} HAZOP nodes to analyze.\")\n",
    "\n",
    "    for node in tqdm(hazop_nodes, desc=\"Analyzing HAZOP Nodes\"):\n",
    "        deviations = engine.deviation_generator.generate_deviations_for_node(node['nodeID'])\n",
    "        for deviation in tqdm(deviations, desc=f\"Deviations for {node['nodeID']}\", leave=False):\n",
    "            analysis = engine.perform_analysis(deviation)\n",
    "            if analysis:\n",
    "                engine.populate_graph_with_analysis(deviation, analysis)\n",
    "\n",
    "    engine.close()\n",
    "    print(\"Phase 4: HAZOP Analysis Engine complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# 05_report_generator.py\n",
    "# This script generates the final, traditional Excel HAZOP report.\n",
    "# It queries the populated HAZOP KG, flattens the data using pandas,\n",
    "# and writes it to a formatted Excel file using xlsxwriter.\n",
    "\n",
    "import config\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "\n",
    "class ReportGenerator:\n",
    "    \"\"\"\n",
    "    Generates a formatted Excel report from the HAZOP KG.\n",
    "    \"\"\"\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def fetch_hazop_data(self):\n",
    "        \"\"\"\n",
    "        Fetches and flattens the HAZOP data from the graph using a Cypher query.\n",
    "        \"\"\"\n",
    "        print(\"Fetching HAZOP data from Knowledge Graph...\")\n",
    "        with self.driver.session() as session:\n",
    "            query = \"\"\"\n",
    "            MATCH (n:HAZOPNode)-[:HAS_DEVIATION]->(d:Deviation)\n",
    "            OPTIONAL MATCH (c:Cause)-[:CAUSES]->(d)\n",
    "            OPTIONAL MATCH (d)-[:LEADS_TO]->(cons:Consequence)\n",
    "            OPTIONAL MATCH (s:Safeguard)-[:PROTECTS_AGAINST]->(d)\n",
    "            \n",
    "            WITH n, d,\n",
    "                 collect(DISTINCT {desc: c.description, conf: c.confidenceLevel, source: c.source}) AS causes,\n",
    "                 collect(DISTINCT {desc: cons.description, conf: cons.confidenceLevel, source: cons.source}) AS consequences,\n",
    "                 collect(DISTINCT {desc: s.description, conf: s.confidenceLevel, source: s.source}) AS safeguards\n",
    "            \n",
    "            RETURN {\n",
    "                node: n.description,\n",
    "                guideword: d.guideword,\n",
    "                parameter: d.parameter,\n",
    "                deviation: d.description,\n",
    "                causes: [c IN causes WHERE c.desc IS NOT NULL],\n",
    "                consequences: [co IN consequences WHERE co.desc IS NOT NULL],\n",
    "                safeguards: [sfg IN safeguards WHERE sfg.desc IS NOT NULL]\n",
    "            } AS result\n",
    "            ORDER BY n.nodeID, d.parameter, d.guideword\n",
    "            \"\"\"\n",
    "            results = session.run(query)\n",
    "            return [record['result'] for record in results]\n",
    "\n",
    "    def generate_excel_report(self, hazop_data, output_path):\n",
    "        \"\"\"\n",
    "        Processes the fetched data and writes it to a formatted Excel file.\n",
    "        \"\"\"\n",
    "        if not hazop_data:\n",
    "            print(\"No HAZOP data found in the graph to generate a report.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Generating Excel report at {output_path}...\")\n",
    "        \n",
    "        flat_data = []\n",
    "        for item in hazop_data:\n",
    "            def format_list(items_list):\n",
    "                return \"\\n\".join([f\"- {i['desc']} (Conf: {i.get('confidenceLevel', 0):.2f}, Src: {i.get('source', 'N/A')})\" for i in items_list])\n",
    "\n",
    "            flat_data.append({\n",
    "                \"Node\": item['node'],\n",
    "                \"Guideword\": item['guideword'],\n",
    "                \"Parameter\": item['parameter'],\n",
    "                \"Deviation\": item['deviation'],\n",
    "                \"Causes\": format_list(item['causes']),\n",
    "                \"Consequences\": format_list(item['consequences']),\n",
    "                \"Safeguards\": format_list(item['safeguards'])\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(flat_data)\n",
    "\n",
    "        with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
    "            df.to_excel(writer, sheet_name='HAZOP Worksheet', index=False)\n",
    "            workbook = writer.book\n",
    "            worksheet = writer.sheets['HAZOP Worksheet']\n",
    "\n",
    "            header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'top', 'fg_color': '#D7E4BC', 'border': 1})\n",
    "            cell_format = workbook.add_format({'text_wrap': True, 'valign': 'top'})\n",
    "            low_confidence_format = workbook.add_format({'bg_color': '#FFC7CE', 'font_color': '#9C0006'})\n",
    "\n",
    "            for col_num, value in enumerate(df.columns.values):\n",
    "                worksheet.write(0, col_num, value, header_format)\n",
    "\n",
    "            worksheet.set_column('A:A', 40, cell_format)\n",
    "            worksheet.set_column('B:D', 15, cell_format)\n",
    "            worksheet.set_column('E:G', 50, cell_format)\n",
    "            worksheet.freeze_panes(1, 0)\n",
    "            \n",
    "            worksheet.conditional_format(f'E2:G{len(df)+1}', {\n",
    "                'type': 'text',\n",
    "                'criteria': 'containing',\n",
    "                'value': '(Conf: 0.',\n",
    "                'format': low_confidence_format\n",
    "            })\n",
    "\n",
    "        print(\"Excel report generated successfully.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function for Phase 5.\"\"\"\n",
    "    reporter = ReportGenerator(config.NEO4J_URI, config.NEO4J_USERNAME, config.NEO4J_PASSWORD)\n",
    "    hazop_data = reporter.fetch_hazop_data()\n",
    "    reporter.generate_excel_report(hazop_data, config.EXCEL_REPORT_PATH)\n",
    "    reporter.close()\n",
    "    print(\"Phase 5: Report Generation complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KG_Builder_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
