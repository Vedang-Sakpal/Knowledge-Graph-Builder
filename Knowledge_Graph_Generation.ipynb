{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86999fd5",
   "metadata": {},
   "source": [
    "# Setup and Install Required Libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1718958",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rdflib owlready2 spacy transformers torch neo4j pandas markdown beautifulsoup4\n",
    "\n",
    "# Download spaCy language model\n",
    "%python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903804cd",
   "metadata": {},
   "source": [
    "# Section 2: Load OntoCAPE ontology \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854d4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes found: 783\n",
      "Example class names: ['N2491f9a091d646bc9bbc050271567271', 'ModelVariableSpecification', 'N17426ab650f745b390869d0b29163c5b', 'Exchange', 'Nc610ab085aa4431fb88992ebf13af6df', 'N3175ebab14504755933b55b25acc0e24', 'IntensiveThermodynamicStateVariable', 'Nec5bae67fa8b42239cf70136cd4443a2', 'SecondLevelSubsystem', 'PhaseInterfaceProperty']\n",
      "Total object properties found: 168\n",
      "Example property names: ['isDefinedBy', 'fulfills', 'isIndexOf', 'has_length', 'isInfluencedBy', 'indicatesMultiplicityOf', 'isPropertyOf', 'hasDirectSubsystem', 'hasFunctionalAspect', 'hasReaction']\n"
     ]
    }
   ],
   "source": [
    "'''from owlready2 import get_ontology\n",
    "\n",
    "onto = get_ontology(\"ontology\\OntoCAPE\\OntoCAPE.owl\").load()\n",
    "\n",
    "# Extract ontology class names and object properties\n",
    "onto_classes = list(onto.classes())\n",
    "onto_labels = [cls.name for cls in onto_classes]\n",
    "onto_obj_props = list(onto.object_properties())\n",
    "\n",
    "print(\"Example OntoCAPE Classes:\", onto_labels[:10])\n",
    "print(\"Example OntoCAPE Object Properties:\", [p.name for p in onto_obj_props[:10]])\n",
    "'''\n",
    "\n",
    "\n",
    "from rdflib import Graph, RDF, RDFS, OWL\n",
    "from urllib.parse import urlparse\n",
    "def extract_name(uri):\n",
    "    return uri.split('#')[-1] if '#' in uri else uri.split('/')[-1]\n",
    "    \n",
    "g = Graph()\n",
    "g.parse('ontology\\OntoCAPE\\OntoCAPE.owl') # Try 'xml' for RDF/XML\n",
    "\n",
    "\n",
    "owl_classes = set(g.subjects(RDF.type, OWL.Class))\n",
    "rdfs_classes = set(g.subjects(RDF.type, RDFS.Class))\n",
    "all_classes = owl_classes.union(rdfs_classes)\n",
    "class_labels = [extract_name(str(c)) for c in all_classes]\n",
    "print(f'Total classes found: {len(class_labels)}')\n",
    "print('Example class names:', class_labels[:10])\n",
    "obj_props = set(g.subjects(RDF.type, OWL.ObjectProperty))\n",
    "obj_labels = [extract_name(str(p)) for p in obj_props]\n",
    "print(f'Total object properties found: {len(obj_labels)}')\n",
    "print('Example property names:', obj_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f31e93",
   "metadata": {},
   "source": [
    "# Section 3: Load and Parse Process Description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e47356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crude Oil Production Unit (COPU) Process Description\n",
      "Overview\n",
      "The crude oil production unit is designed to process, separate, and condition crude oil from field production, separating associated gas and produced water, and preparing the oil for storage and export. The unit has a nominal capacity of 10 Mbpd (million barrels per day) and handles crude oil with an API gravity of 33Â°.\n",
      "Main Process Sections\n",
      "1. Incoming Crude Oil and Initial Separation\n",
      "\n",
      "Field Production Feed: \n",
      "Crude oil from wells arr\n"
     ]
    }
   ],
   "source": [
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load .md file\n",
    "with open(\"descriptions\\HAZOP_011_process_description.md\", \"r\") as f:\n",
    "    md_text = f.read()\n",
    "\n",
    "html = markdown.markdown(md_text)\n",
    "text = BeautifulSoup(html, \"html.parser\").get_text()\n",
    "\n",
    "print(text[:500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd1847",
   "metadata": {},
   "source": [
    "# Section 4: Extract Entities and Relations from Text\n",
    "We assume a basic rule-based or LLM-based approach here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e8714",
   "metadata": {},
   "source": [
    "## Using keyword based rules to extract entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c240db7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found equipment: pump in line -> The treated oil is stored in tanks before being pumped to the custody transfer point for export[1].\n",
      "Found equipment: pump in line -> The treated water is pumped and re-injected into a disposal well[1].\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example: rule-based extraction\n",
    "equipment_keywords = [\"reactor\", \"pump\", \"heat exchanger\", \"distillation\", \"compressor\", \"column\"]\n",
    "connections = []\n",
    "\n",
    "for line in text.split(\"\\n\"):\n",
    "    for kw in equipment_keywords:\n",
    "        if kw in line.lower():\n",
    "            print(f\"Found equipment: {kw} in line -> {line}\")\n",
    "\n",
    "# You can also extract relations manually or use spaCy/transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea311f",
   "metadata": {},
   "source": [
    "## Using Spacy/transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec3bb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\anaconda3\\envs\\KG_Builder_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: {'Tank', 'Pump'}\n",
      "Relations: [('Pump', 'TRANSFERS_TO', 'Tank'), ('Tank', 'TRANSFERS_TO', 'Pump')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load transformer for relation extraction\n",
    "relation_extractor = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Custom equipment types (can be extended)\n",
    "equipment_types = [\n",
    "    \"reactor\", \"heat exchanger\", \"pump\", \"distillation column\",\n",
    "    \"compressor\", \"valve\", \"vessel\", \"tank\"\n",
    "]\n",
    "\n",
    "# Entity & relation extraction\n",
    "doc = nlp(text)\n",
    "entities = set()\n",
    "relations = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    s_text = sent.text.strip()\n",
    "    for eq1 in equipment_types:\n",
    "        for eq2 in equipment_types:\n",
    "            if eq1 != eq2 and eq1 in s_text.lower() and eq2 in s_text.lower():\n",
    "                # Use zero-shot to extract relation type\n",
    "                candidate_labels = [\"feeds\", \"cools\", \"heats\", \"connects to\", \"transfers to\"]\n",
    "                result = relation_extractor(s_text, candidate_labels)\n",
    "                rel = result['labels'][0]\n",
    "                relations.append((eq1.title(), rel.replace(\" \", \"_\").upper(), eq2.title()))\n",
    "                entities.add(eq1.title())\n",
    "                entities.add(eq2.title())\n",
    "\n",
    "print(\"Entities:\", entities)\n",
    "print(\"Relations:\", relations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598223c9",
   "metadata": {},
   "source": [
    "# Section 5: Map to OntoCAPE Ontology Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ecf6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each entity to an OntoCAPE class\n",
    "equipment_to_ontocape = {\n",
    "    \"Reactor\": \"ontoCAPE.ChemicalReactor\",\n",
    "    \"Heat Exchanger\": \"ontoCAPE.HeatExchanger\",\n",
    "    \"Pump\": \"ontoCAPE.Pump\",\n",
    "    \"Distillation Column\": \"ontoCAPE.DistillationColumn\",\n",
    "    \"Compressor\": \"ontoCAPE.Compressor\",\n",
    "    \"Valve\": \"ontoCAPE.Valve\",\n",
    "    \"Vessel\": \"ontoCAPE.Vessel\",\n",
    "    \"Tank\": \"ontoCAPE.Tank\"\n",
    "}\n",
    "\n",
    "nodes = [(ent, equipment_to_ontocape.get(ent, \"ontoCAPE.Equipment\")) for ent in entities]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf6b15",
   "metadata": {},
   "source": [
    "# Section 6: Build Graph Model (Nodes + Relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb46e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cypher_node(name, label):\n",
    "    return f\"MERGE (:{label.split('.')[-1]} {{name: '{name}'}})\"\n",
    "\n",
    "def to_cypher_relation(src, rel, tgt):\n",
    "    return f\"MATCH (a {{name: '{src}'}}), (b {{name: '{tgt}'}}) MERGE (a)-[:{rel}]->(b)\"\n",
    "\n",
    "cypher_nodes = [to_cypher_node(n, l) for n, l in nodes]\n",
    "cypher_edges = [to_cypher_relation(s, r, t) for s, r, t in relations]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9b7315",
   "metadata": {},
   "source": [
    "# Section 7: Generate Cypher Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27b969b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGE (:Tank {name: 'Tank'})\n",
      "MERGE (:Pump {name: 'Pump'})\n",
      "MATCH (a {name: 'Pump'}), (b {name: 'Tank'}) MERGE (a)-[:TRANSFERS_TO]->(b)\n",
      "MATCH (a {name: 'Tank'}), (b {name: 'Pump'}) MERGE (a)-[:TRANSFERS_TO]->(b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Save generated Cypher code\\nwith open(\"generated_kg.cypher\", \"w\") as f:\\n    f.write(\"// Nodes\\n\")\\n    f.write(\"\\n\".join(cypher_nodes))\\n    f.write(\"\\n\\n// Relationships\\n\")\\n    f.write(\"\\n\".join(cypher_edges))\\n\\nprint(\"Cypher file \\'generated_kg.cypher\\' saved successfully.\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_cypher_node(name, label):\n",
    "    return f\"MERGE (:{label} {{name: '{name}'}})\"\n",
    "\n",
    "def to_cypher_relation(src, rel, tgt):\n",
    "    return f\"MATCH (a {{name: '{src}'}}), (b {{name: '{tgt}'}}) MERGE (a)-[:{rel.upper()}]->(b)\"\n",
    "\n",
    "cypher_nodes = [to_cypher_node(n, l.split(\".\")[-1]) for n, l in nodes]\n",
    "cypher_edges = [to_cypher_relation(s, r, t) for s, r, t in relations]\n",
    "\n",
    "# Print Cypher code\n",
    "for line in cypher_nodes + cypher_edges:\n",
    "    print(line)\n",
    "\n",
    "'''\n",
    "# Save generated Cypher code\n",
    "with open(\"generated_kg.cypher\", \"w\") as f:\n",
    "    f.write(\"// Nodes\\n\")\n",
    "    f.write(\"\\n\".join(cypher_nodes))\n",
    "    f.write(\"\\n\\n// Relationships\\n\")\n",
    "    f.write(\"\\n\".join(cypher_edges))\n",
    "\n",
    "print(\"Cypher file 'generated_kg.cypher' saved successfully.\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KG_Builder_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
